# Medición y optimización del rendimiento del modelo con ROC y AUC

Las curvas de características operativas del receptor son una manera eficaz de evaluar y ajustar los modelos de clasificación entrenados. 
Presentamos y explicamos la utilidad de estas curvas a través de contenido de aprendizaje y ejercicios prácticos.

## Introduccion
Podemos evaluar nuestros modelos de clasificación en términos de los tipos de errores que cometen, como falsos negativos y falsos positivos. Esto puede proporcionar información sobre los tipos de errores que comete un modelo, pero no proporciona necesariamente información detallada sobre qué rendimiento podría tener el modelo si se realizaran pequeños ajustes según sus criterios de decisión. Aquí trataremos las curvas de características operativas del receptor (ROC), que se basan en la idea de una matriz de confusión, pero nos proporcionan información más detallada que nos permite mejorar nuestros modelos en mayor medida.


Escenario:
A lo largo de este módulo, usaremos el siguiente escenario de ejemplo para explicar y hacer prácticas de trabajo con las curvas de ROC.

Su organización benéfica para rescatar personas tras una avalancha ha creado con éxito un modelo de Machine Learning que puede estimar si un objeto detectado por sensores ligeros es un excursionista o un objeto natural, como un árbol o una roca. Esto le permite realizar un seguimiento del número de personas que se encuentran en la montaña con el fin de saber si se necesita un equipo de rescate tras producirse una avalancha. El modelo funciona razonablemente bien, aunque se pregunta si hay espacio para la mejora. Internamente, el modelo debe tomar una decisión binaria sobre si un objeto es un excursionista o no, pero esto se basa en probabilidades. ¿Se puede retocar este proceso de toma de decisiones para mejorar su rendimiento?

## Analisis de la clasificacion con curvas de caracteristicas operativas del receptor

Los modelos de clasificación deben asignar un ejemplo a una categoría. Por ejemplo, debe usar características como el tamaño, el color y el movimiento para determinar si un objeto es un excursionista o un árbol.


Podemos asegurarnos de que nuestros datos están equilibrados, limpios y escalados. También podemos modificar nuestra arquitectura de modelo y usar hiperparámetros para obtener tanto rendimiento como podamos de nuestros datos y nuestra arquitectura. Finalmente, no encontramos ninguna manera mejor de mejorar el rendimiento en nuestro conjunto de pruebas (o datos de exclusión) y declarar que nuestro modelo está listo.

El ajuste del modelo a este punto puede ser complejo, pero se puede usar un sencillo paso final para mejorar aún más el buen funcionamiento de nuestro modelo. Sin embargo, para entender esto, es necesario volver a los aspectos básicos.

### Probabilidades y categorias
Muchos modelos tienen varias fases de toma de decisiones y la última suele ser simplemente un paso de binarización. Durante la binarización, las probabilidades se convierten en una etiqueta dura.
Por ejemplo, supongamos que el modelo se proporciona con características y calcula que existe una probabilidad del 75 % de que se mostrara un excursionista y una probabilidad del 25 % de que se mostrara un árbol. Un objeto no puede ser 75 % excursionista y 25 % árbol, es una cosa o la otra. Como tal, el modelo aplica un umbral, que normalmente es del 50 %. Como la clase correspondiente al excursionista es mayor que el 50 %, se declara que el objeto es un excursionista.

El umbral del 50 % es lógico, lo que significa que siempre se elige la etiqueta más probable según el modelo. Sin embargo, si el modelo está sesgado, es posible que este umbral del 50 % no sea adecuado. Por ejemplo, si el modelo tiene una ligera tendencia a elegir árboles más que excursionistas (la frecuencia a la hora de elegir árboles es un 10 % mayor que la normal), podríamos ajustar nuestro umbral de decisión para tener esto en cuenta.

### Actualizacion en funcion de las matrices de decision

Las matrices son una manera de evaluar los tipos de errores que comete el modelo.

[](https://learn.microsoft.com/es-es/training/modules/optimize-model-performance-roc-auc/media/2-decision-matrices.png)

Podemos calcular algunas caracteristicas utiles a partir de matriz.

* Tasa de verdaderos positivos(sensibilidad), es la frencuencia en que las etiquetas verdaderas se identifican correctamente como verdaderas
* Tasa de falsos (tasa falsa de alarmas), es la frecuencia en que las etiquetas falsas se indentificar incorrectamente como verdaderas

El analisis de estas nos puede ayudar a comprender el rendimiento del modelo.

### Curcas ROC
Curvas operativas del receptor, son un grafico que traza la tasa de verdaderos positivos frente a la tasa de falsos positivos.

Las curvas de ROC pueden resultar confusas para principiantes por dos motivos principales. El primero es que los principiantes saben que un modelo solo tiene un valor para las tasas de verdaderos positivos y verdaderos negativos. Por lo tanto, un trazado de ROC debe tener este aspecto:

[](https://learn.microsoft.com/es-es/training/modules/optimize-model-performance-roc-auc/media/roc-graph.png)

Esto es correcto, pero los modelos tienen un umbral del 50% la mayoria de veces, si recalculamos el umbral a un 30% obtenemos otro punto

[](https://learn.microsoft.com/es-es/training/modules/optimize-model-performance-roc-auc/media/roc-graph-2.png)

si modificamos el umbral varias veces del 0 al 100%
[](https://learn.microsoft.com/es-es/training/modules/optimize-model-performance-roc-auc/media/roc-graph-3.png)

trazando la linea queda

[](https://learn.microsoft.com/es-es/training/modules/optimize-model-performance-roc-auc/media/roc-graph-4.png)


## Comparación y optimización de curvas de ROC
Las curvas nos permiten comparar modelos entre sí y ajustar nuestro modelo seleccionado.

### Ajuste de un modelo
El uso más obvio de una curva de ROC es elegir un umbral de decisión que proporcione el mejor rendimiento.
Hemos visto en el ejercicio anterior que, al construir una curva de ROC, solo estamos cambiando el umbral de decisión y evaluando el funcionamiento del modelo. Al hacerlo, podemos encontrar el umbral que proporciona los resultados óptimos.
Esto significa que el umbral óptimo depende de lo que se intente lograr. 

### Comparacion de modelos con AUC
Se pueden usar para comparar modelos entre sí, al igual que las funciones de costo. La curva de ROC de un modelo muestra lo bien que funcionará para una variedad de umbrales de decisión.
Al final del día, lo más importante de un modelo es qué rendimiento tendrá en el mundo real, donde solo hay un umbral de decisión. ¿Por qué, entonces, desearíamos comparar modelos con umbrales que nunca usaremos? Existen dos respuestas para esto.

En primer lugar, comparar curvas de ROC de maneras concretas es como realizar una prueba estadística que nos indique no solo que un modelo ha funcionado mejor en este conjunto de pruebas concreto, sino si es probable que siga teniendo un mejor rendimiento en el futuro. Esto está fuera del ámbito de este material de aprendizaje, pero merece la pena tenerlo en cuenta.

En segundo lugar, la curva de ROC muestra, hasta cierto punto, cómo depende el modelo de tener el umbral perfecto. Por ejemplo, si nuestro modelo solo funciona bien cuando tenemos un umbral de decisión de 0,9, pero muy por encima o por debajo de este valor, no es un buen diseño. Probablemente preferiríamos trabajar con un modelo que funcione razonablemente bien para varios umbrales, sabiendo que si los datos reales que encontramos son ligeramente diferentes a nuestro conjunto de pruebas, el rendimiento de nuestro modelo no se contraerá necesariamente.

### ¿Como se comparan las ROC?
La manera más fácil de comparar las ROC numéricamente es usar el área bajo la curva (AUC). Literalmente, se trata del área del gráfico que se encuentra por debajo de la curva.

[](https://learn.microsoft.com/es-es/training/modules/optimize-model-performance-roc-auc/media/roc-auc-graph.png)

Aunque nuestro modelo cuyo rendimiento no era mejor al del simple azar tenga un área de aproximadamente 0,5:

[](https://learn.microsoft.com/es-es/training/modules/optimize-model-performance-roc-auc/media/roc-auc-graph-2.png)

Cuanto más perfecto sea un modelo, mayor será esta área. Si tenemos un modelo con una AUC grande, sabemos que funciona bien para diversos umbrales, por lo que probablemente tiene una buena arquitectura y ha recibido un buen entrenamiento. Por el contrario, un modelo con una AUC pequeña (más próxima a 0,5) no funciona bien.


