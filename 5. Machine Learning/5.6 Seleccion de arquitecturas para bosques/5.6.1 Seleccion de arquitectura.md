# Selección y personalización de arquitecturas e hiperparámetros mediante bosques aleatorio

Exploramos cómo la modificación de la arquitectura de modelos más complejos puede lograr resultados más eficaces.

## Introduccion
No todos los modelos se pueden representar como una linea recta, hay modelos complejos se consideran mas como graficos de flujo o estructuras de programacion tradicionales.
Estos suelen contar con niveles adicionales de personalizacion, lo que los hace mas eficaces y mas complicados.

**Escenario: Predicción de resultados olímpicos mediante aprendizaje automático**
A lo largo de este módulo, haremos referencia al siguiente escenario de ejemplo cuando expliquemos los conceptos relacionados con la arquitectura de modelos y los hiperparámetros. Aunque este escenario está diseñado para que parezca complejo al principio, a medida que avancemos por los ejercicios veremos cómo se puede abordar con un poco de experimentación y pensamiento crítico.

El eslogan de los Juegos Olímpicos consta de tres palabras en latín: Citius, Altius, Fortius. Estas palabras significan Más rápido, Más alto, Más fuerte. Desde que se estableció este eslogan, la variedad de juegos de las Olimpiadas ha crecido enormemente para incluir el tiro, la vela y los deportes de equipo. Nos gustaría explorar el papel que siguen jugando las características físicas básicas a la hora de predecir quién gana una medalla en uno de los eventos deportivos más prestigiosos del planeta. Para ello, exploraremos la gimnasia rítmica: una incorporación moderna a los juegos que combina el baile, la gimnasia y la calistenia. Podría caber esperar que las características básicas de edad, altura y peso solo desempeñan un papel limitado, dada la necesidad de agilidad, flexibilidad, destreza y coordinación. Vamos a usar algunos modelos de aprendizaje automático más avanzados para ver lo críticos que son realmente estos factores básicos.

## Arboles de decision y arquitectura de modelos
En el aprendizaje automático, la arquitectura hace referencia a un concepto similar. ¿Cuántos parámetros tiene y cómo están vinculados para lograr un cálculo? ¿Realizamos muchos cálculos en paralelo (ancho) o tenemos operaciones en serie que se basan en un cálculo anterior (profundidad)? ¿Cómo se pueden proporcionar entradas a este modelo y cómo se pueden recibir salidas? Estas decisiones de arquitectura solo se aplican normalmente a modelos más complejos y pueden variar desde las más sencillas a las más complejas. Estas decisiones se suelen tomar antes de entrenar el modelo, aunque, en algunas circunstancias, hay margen para realizar cambios después del entrenamiento.

### ¿Que es un arbol de decision?
Es un diagrama de flujo.
Es un modelo de categorizacion que divide las decisiones en varios pasos.

[Arbol de decisiones](https://learn.microsoft.com/es-es/training/modules/machine-learning-architectures-and-hyperparameters/media/7-2-a.jpg)

### ¿Como se entrenan los arboles de decisiones?
Se entrenan un nodo o punto de decision a la vez.
En el primer nodo, se evalúa todo el conjunto de entrenamiento. A partir de ahí, se selecciona una característica que pueda separar mejor el conjunto en dos subconjuntos que tengan etiquetas más homogéneas. Por ejemplo, imagine que nuestro conjunto de entrenamiento fuera el siguiente:

|Peso (característica)	|Edad (característica)	|Ganó una medalla (etiqueta)
|-----------------------|-----------------------|--------------------------
|90	|18	|No
|80	|20	|No
|70	|19	|No
|70	|25	|No
|60	|18	|Sí
|80	|28	|Sí
|85	|26	|Sí
|90	|25	|Sí

si queremos una regla sencilla que divida todos los datos, podriamos hacerlo por edad en torno a 24 años (La mayoria de medallistas tenian mas de 24), esta division nos proporcionaria dos subconjuntos de datos.

Subconjunto 1

|Peso (característica)|	Edad (característica)|	Ganó una medalla (etiqueta)
|---------------------|----------------------|------------------------------
|90	|18	|No
|80	|20	|No
|70	|19	|No
|60	|18	|Sí

Subconjunto 2

|Peso (característica)|	Edad (característica)|	Ganó una medalla (etiqueta)
|---------------------|----------------------|------------------------------
|70	|25	|No
|80	|28	|Sí
|85	|26	|Sí
|90	|25	|Sí

Si nos detenemos aqui tenemos un modelo sencillo con un nodo y dos hojas.
La hoja 1 contiene los que no ganaron medalla y tiene una precision de 75%
La hoja 2 contiene a los medallitas y tambien tiene una precision del 75%

Podemos seguir el proceso y dividir aun mas.

El subconjunto 1 puede dividirse por el peso, por ejemplo peso<65, para predecir que las personas con peso mayor de 65 han ganado medalla.

El subconjunto 2, tambien puede divirse por el peso, cualquier persona con peso<70 ha ganado medalla.

Esto nos proporcionaria un arbol que podria lograr un 100% de precision.

#### Puntos fuertes y debiles de los arboles de decisiones
Se considera que los árboles de decisión tienen un sesgo bajo. Esto significa que suelen ser buenos para identificar características que son importantes para etiquetar algo correctamente

La debilidad es el sobreajuste.

#### La arquitectura del modelo afecta el sobreajuste.

La forma en que estructuramos el arbol de decision es clave para evitar los puntos debiles.
Cuanto mas profundo sea el arbol mas probabilidad de sobreajuste


## Bosques aleatorios y seleccion de arquitecturas
La experimentacion con arquitecturas suele centrarse en la creacion de modelos modernos eficaces.

Considderar los arboles de decisiones desde una perspectiva mas amplia dio lugar a una arquitectura de modelos muy reconocida que reduce la tendencia de sus arboles de decisiones a sobreajusrtar los datos.

### ¿Que es un bosque aleatorio?
Es una coleccion de arboles de decisiones,que se usan en conjunto para calcular que etiqueta se debe asignar a una muestra.

Ejemplo: Si entrenamos un bosque para determinar los medallistas con 100 arboles usariamos cada arbol de manera independiente, donde cada uno votaria, lo que proporciona una decision final.

#### ¿Como se entrena un bosque aleatorio?
Los bosques se construyen bajo  la idea de que al tener un unico arbol que este sobreajustado o sesgado al entrenar varios este sesgo variara, por ende se entrenan de manera independiente con conjuntos de datos ligeramente diferentes.

Para entrenar un único árbol de decisión, se extrae un determinado número de muestras (atletas en nuestro escenario) del conjunto de entrenamiento completo. Cada muestra se puede seleccionar más de una vez y esta operación se realiza aleatoriamente.

#### Ventajas del bosque aleatorio
El rendimiento puede compararse con los de las redes neuronales, son mas faciles que entrenar, no necesitan grandes conjuntos de datos.

#### Desventaja del bosque aleatorio
Son dificiles de comprender.

### ¿Como puedo personalizar estas arquitecturas?
Los bosques aleatorios tienen varias opciones de arquitectura.
Los mas facil es tener en cuenta, el tamaño del bosque, el numero de arboles que intervienen, junto al tamaño de los arboles.
Aumentar el tamaño de un arbol, da la posibilidad de sobreajuste, pero en los bosques esto se puede manejar aumentando la cantidad de arboles.

También se puede restringir cada árbol a un número determinado de características, o no permitir la creación de hojas cuando solo haya una diferencia marginal en el rendimiento del entrenamiento. La capacidad de un bosque aleatorio para realizar buenas predicciones no es infinita. En algún momento, aumentar el tamaño y el número de árboles no ofrece ninguna mejora adicional debido a la variedad limitada de datos de entrenamiento de los que disponen.


## Hiperparametros en la clasificacion
Son valores de configuracion para el entrenamiento, estos afectan el entrenamiento e influyen en el rendimiento.


### Bosques aleatorios como ejemplo
La linea entre los hiperparametros de decisiones de arquitectura puede ser difusa, ya que estos afectan a los parametros dentro del modelo y tambien a como se estructuran los arboles y bosques.

Recuerde que, al principio del entrenamiento, a cada árbol de decisión se le proporcionan numerosas muestras, como 100 gimnastas, algunos de las cuales ganaron medallas. Se debe crear un árbol que divida progresivamente estas muestras en subgrupos de atletas más pequeños. El objetivo es que estos subgrupos contengan atletas que sean iguales, por ejemplo, dentro de cada subgrupo, todos los atletas ganaron medallas o todos no ganaron medallas. Vamos a explorar algunos hiperparámetros que pueden afectar a este proceso de entrenamiento

#### Criterios de division
Durante el entrenamiento, el optimizador decide en cuanto dividir un nodo.
Se toma el metodo de hiperparametro.
Los metodos comunes de dividir nosos se basan en la teoria de la informacion, donde se divide una muestra para que las resultantes sean mas puras que la original.

#### Disminucion minima de impurezas
Aplica que un nodo solo se puede dividir si mejora el modelo algo o bastante.
Hay varios hiperparámetros relacionados que pueden bloquear la creación de nuevos nodos, como la profundidad máxima o el número mínimo de muestras en un nodo.
Restringir la complejidad de un árbol puede reducir su tendencia al sobreajuste.


#### Numero maximo de caracteristicas
Cuando se crean arboles se les proporciona un subconjunto de datos al que ajustarse y una lista de caracteristicas.
Al aumentar el numero maximo de  caracteristicas de cada arbol se puede mejorar la capacidad de cada uno para ajustarse a un conjunto de entrenamiento, ya que proporciona mas informacion.

El motivo es que proporcionar siempre muchas características puede significar que los árboles del bosque terminen por ser más parecidos entre sí, lo que reduce la ventaja de un bosque aleatorio sobre un árbol de decisión simple. Encontrar el equilibrio entre estos extremos normalmente requiere cierta experimentación.

### Propagacion
El ajuste del modelo suele basarse en numeros aleatorios.
Los equipos no generan números realmente aleatorios, sino que contienen reglas que determinan cómo generar una lista de números aleatorios, dado un número inicial, denominado valor de inicialización aleatorio.

En el aprendizaje automático, usamos números aleatorios para inicializar los parámetros del modelo o para dividir los conjuntos de datos en conjuntos de entrenamiento y pruebas. Si se establece el valor de inicialización aleatorio, los valores aleatorios usados durante el proceso de entrenamiento serán los mismos cada vez que se vuelva a ejecutar el código. Lo que significa que, cada vez que se vuelva a ejecutar el código, se asignarán los mismos datos a los conjuntos de entrenamiento o prueba y a los modelos de entrenamiento que tengan el mismo estado de inicialización (parámetros iniciales).

Por el contrario, si no establecemos el valor de inicialización, el equipo seleccionará uno por nosotros, por ejemplo, en función del tiempo, lo que significa que la ejecución de nuestro entrenamiento dos veces puede dar resultados ligeramente diferentes.

La inicialización aleatoria no es estrictamente un hiperparámetro, pero lo presentamos aquí para resaltar que este parámetro externo puede desempeñar un papel en la eficacia del entrenamiento. Aunque suele ser menor, si el modelo es muy complejo o la cantidad de datos disponibles es pequeña, el rendimiento del conjunto de pruebas del modelo puede ser marcadamente diferente si se usan dos valores de inicialización distintos. En tales situaciones, a menudo se paga por ejecutar el entrenamiento con varios valores de inicialización diferentes a fin de evaluar hasta qué punto el diseño del modelo es adecuado y hasta qué punto el rendimiento es simplemente cuestión de suerte.





